\documentclass[a4paper,12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsfonts,stmaryrd}
\usepackage{soul}
\usepackage{array}
\usepackage{empheq}
\usepackage{xfrac}
\usepackage{minibox}
\usepackage{enumitem}
	\setlist{nosep} % or \setlist{noitemsep} to leave space around whole list
\usepackage{color}
\usepackage{blkarray}
\setcounter{MaxMatrixCols}{20}

\newcommand{\tcb}{\textcolor{blue}}
\newcommand{\todo}[1]{\textcolor{red}{[TODO\@: #1]}}



\begin{document}
\allowdisplaybreaks


% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\section{Space-time discretizations}\label{sec:disc}

Consider a linear PDE of the form 
\begin{align*}
u_t + \mathcal{L}(u,\mathbf{x}) = g(\mathbf{x},t).
\end{align*}
We discretize $\mathcal{L}$ in space and, denoting $\mathbf{u}(t)$ the discrete solution in space at time $t$
and $\mathbf{g}(t)$ the corresponding right-hand side, we arrive at the system of ODEs 
%
\begin{align}\label{eq:ode}
\mathbf{u}_t + \mathcal{L}(t)\mathbf{u} = \mathbf{g}(t).
\end{align}
%
Note, in finite element discretizations, there will also be a mass matrix term:
%
\begin{align}\label{eq:odeM}
M\mathbf{u}_t + \mathcal{L}(t)\mathbf{u} = \mathbf{g}(t).
\end{align}
%
Time integrators can be largely broken down into Runge-Kutta-type methods and linear multistep methods. Runge-Kutta methods
are one-step, $s$-stage methods, and are addressed in Section \ref{sec:RK}. Linear multistep methods are addressed in Section
\ref{sec:multi}.

% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\section{Linear multistep methods}\label{sec:multi}

Three main classes of linear multistep methods are the Adams-Bashforth (explicit), Adams-Moulton (implicit), and backwards
differentiation formulas (BDF) (implicit), all of which can be naturally framed as a large sparse linear system. Suppose we have a
discrete time grid with time step $\delta t$ and time points $\{t_0,t_1,...\}$. Denote $\mathbf{g}(t_n) = \mathbf{g}_n$ and
$\mathcal{L}(t_n) = \mathcal{L}_n$. The first three formulae for each of the linear multistep classes of integrators in the context
of \eqref{eq:odeM} are given as follows:
%
{\footnotesize
\begin{align*}
%
\textnormal{\textbf{Adams-Bashforth:}}\hspace{50.5ex}&\\
\tfrac{1}{\delta t}M\mathbf{u}_n - (\tfrac{1}{\delta t}M - \mathcal{L}_{n-1})\mathbf{u}_{n-1} & =\mathbf{g}_{n-1}  \\
\tfrac{1}{\delta t}M\mathbf{u}_n - (\tfrac{1}{\delta t}M - \tfrac{3}{2}\mathcal{L}_{n-1})\mathbf{u}_{n-1} -
	\tfrac{1}{2}\mathcal{L}_{n-2}\mathbf{u}_{n-2} & = \tfrac{3}{2}\mathbf{g}_{n-1} - \tfrac{1}{2}\mathbf{g}_{n-2}\\
\tfrac{1}{\delta t}M\mathbf{u}_n - (\tfrac{1}{\delta t}M - \tfrac{23}{12}\mathcal{L}_{n-1})\mathbf{u}_{n-1} -
	\tfrac{4}{3}\mathcal{L}_{n-2}\mathbf{u}_{n-2} + \tfrac{5}{12}\mathcal{L}_{n-3}\mathbf{u}_{n-3} & = \tfrac{23}{12}\mathbf{g}_{n-1} -
	\tfrac{4}{3}\mathbf{g}_{n-2} + \tfrac{5}{12}\mathbf{g}_{n-3}\\
%
\textnormal{\textbf{Adams-Moulton:}}\hspace{52ex}&\\
(\tfrac{1}{\delta t}M +  \mathcal{L}_n)\mathbf{u}_n - \tfrac{1}{\delta t}M\mathbf{u}_{n-1} &  =   \mathbf{g}_n \\
(\tfrac{1}{\delta t}M + \tfrac{1}{2} \mathcal{L}_n)\mathbf{u}_n - (\tfrac{1}{\delta t}M - \tfrac{1}{2} \mathcal{L}_{n-1})\mathbf{u}_{n-1} & =
	\tfrac{1}{2}  (\mathbf{g}_n + \mathbf{g}_{n-1}) \\
(\tfrac{1}{\delta t}M + \tfrac{5}{12} \mathcal{L}_n)\mathbf{u}_n - (\tfrac{1}{\delta t}M - \tfrac{2}{3} \mathcal{L}_{n-1})\mathbf{u}_{n-1} - 
	\tfrac{1}{12}\mathcal{L}_{n-2}\mathbf{u}_{n-2}& = \tfrac{5}{12} \mathbf{g}_n + \tfrac{2}{3} \mathbf{g}_{n-1} -
	\tfrac{1}{12} \mathbf{g}_{n-2}\\
%
\textnormal{\textbf{BDF:}}\hspace{65ex}&\\
(\tfrac{1}{\delta t}M + \mathcal{L}_n)\mathbf{u}_n - \tfrac{1}{\delta t}M\mathbf{u}_{n-1} &  =   \mathbf{g}_n \\
(\tfrac{1}{\delta t}M + \tfrac{2}{3} \mathcal{L}_n)\mathbf{u}_{n} - \tfrac{4}{3\delta t}M\mathbf{u}_{n-1} +
 	\tfrac{1}{3\delta t}M\mathbf{u}_{n-2} & = \tfrac{2}{3}\mathbf{g}_n \\
(\tfrac{1}{\delta t}M + \tfrac{6}{11} \mathcal{L}_n)\mathbf{u}_{n} - \tfrac{18}{11\delta t}\mathbf{u}_{n-1} +
	\tfrac{9}{11\delta t}M\mathbf{u}_{n-2} - \tfrac{2}{11\delta t}M\mathbf{u}_{n-3} +  & =   \tfrac{6}{11}  \mathbf{g}_n.
\end{align*}
}
%
Here, we do not multiply through by $\delta t$ because $M \sim dx^k$ for some power. Maintaining the product of $M$ and
$\delta t$ makes sense to keep $O(1)$ operators. We also do not invert $M$ and pass it through because a mass matrix is typically
not diagonal, and it is not clear when lumping is a good option. In the case of, e.g., finite differences, $M = C\cdot I$, where
$C$ is some constant related to the spatial discretization, e.g., $C = \tfrac{1}{dx^2}$.

Full space-time discretizations take the form of a block lower triangular matrix, $\mathcal{A}$, where each block corresponds to the full
spatial solution at one point in time. The Adams-Bashforth methods are explicit, so the block diagonal of $\mathcal{A}$ is the mass matrix,
and off-diagonal blocks given by spatial discretization terms at previous time steps. Adams-Moulton have spatial discretization blocks
along the diagonal as well as off-diagonals. Finally, BDF methods only have the spatial discretization along the diagonal; off-diagonal
blocks consist only of the mass matrix. Such a structure reduces the number of matrix nonzeros and connectivity compared with an
Adams-type scheme; however, the Adams-Moulton implicit methods gain an extra order of accuracy. For example, if we consider
two time-neighbors away, that is, approximate $\mathbf{u}_i$ using $\mathbf{u}_{i-1}$ and $\mathbf{u}_{i-2}$, BDF is $O(\delta t^2)$
accurate, while Adams-Moulton is $O(\delta t^3)$. 

% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\subsection{Matrix structure}

Are these matrices M-matrices?? May help motivate AMG.. If $\mathcal{L}$ is an M-matrix, BDF1/AM1 also is, but none of the others.
M-matrix must have non-positive off-diagonals.

If $\mathcal{L}$ is symmetric, $A^*A$ and $AA^*$ are almost identical, thus (hopefully) implying the left and right singular vectors are
similar. This would motivate a $P^*AP$ approach, which seems to be working well on diffusion. For BDF-type schemes, $A^*A$ and $AA^*$
seem to differ only in the first and last $k$ time steps, where $k$ is the order of the time integration.

Real symmetric matrix has SVD equivalent to eigendecomposition, with signs swapped on 




% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\subsection{BDF}


% ------------------------------------------------------------------------------------------------------- %
\subsubsection{Assume $N_p | n_t$}
% MPI ranks are zero-indexed. 

For an initial implementation, assume that $N_p < n_t$ and $N_p | n_t$ ($N_p$ divides $n_t$), where $P_T = \frac{n_t}{N_p}$. This is nice
because no time steps (and the corresponding matrix blocks) are split over multiple processors. That is, each processor will create its own
spatial discretizaion(s), and we do not need to split these over processors. Assume we are using zero-indexing. 

Structurally, the BDF discretizations are fairly straightforward. The right-hand side consists of $\mathbf{g}$ evaluated at the current time
and scaled by a constant. The diagonal block of the matrix consists of the identity plus some constant times the spatial discretization, and
there are $k$ off-diagonal blocks, each of which are a constant-diagonal matrix, where $k$ is the order of accuracy of the BDF discretization. 
Initial conditions, $\mathbf{u}_{-1} = f(\mathbf{x})$ are also easily satisfied by moving to the right-hand-side. Thus each block row of the matrix
takes on the CSR structure of the spatial discretization, with an extra $k$ nonzeros per row (except for points near the boundary).\\
\\
\noindent\textbf{Notes on construction:}
\begin{itemize}
\item For time $t_i$, the $i$th block row consists of rows $\{in_{\mathbf{x}}, ...,[(i+1)n_{\mathbf{x}}-1]\}$ in the matrix. 
\item Processor $p_{\ell}$ has time steps $\{P_T\ell,...,P_T(\ell+1)-1\}$. 
\item When adding spatial discretization matrices to the global structure, we will need to adjust the column indices. The $j$th block column
will have column indices $+jn_{\mathbf{x}}$. 
\item Suppose $\frac{n_t}{N_p} = q$. Then we will have $q$ time steps (i.e., block rows) stored on each processor and $qn_{\mathbf{x}}$
total rows. 
\item At time $t_0$ (first row in $\mathcal{A}$), there is only a diagonal block, consisting of the spatial discretization. Right hand side must
account for initial conditions. \tcb{How do we handle first time step(s) with high-order BDF scheme?}

\end{itemize}


% ------------------------------------------------------------------------------------------------------- %
\subsubsection{Assume $N_p > n_t$ and $n_t | N_p$}
% MPI ranks are zero-indexed. 

Now suppose there are more processors than time steps. For a large, 2d- or 3d-space problem, this is entirely possible.
Here we make a similar assumption as above, this time that $n_t | N_p$. Then, one time step will be split over multiple
processors. Let $N_p$ be the number of processors, $n_t$ the number of time steps, and $N_{p_x} = \frac{N_p}{n_t}$
be the number of processors per time index (i.e., number of processors per spatial discretization). For rank $r_i$, we
define communication groups 
\begin{align*}
\mathcal{G}_i = \left\{r_k \text{ $\Big|$ } \left\lfloor \frac{r_k}{N_{p_x}} \right\rfloor == i\right\}.
\end{align*}
These groups will be the communication packages that we build spatial discretizations over. 



% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\subsection{Boot-strapping}

For high-order integrators, there needs to be some sort of boot-strapping process such that the initial time step has sufficient
information to be high-order accuracy. One option is to take a very small time step, for example, use BDF1 to take a step of
size $\delta t^2$, and proceed to use the initial condition and the solution at time $\delta t^2$ to take a time step of size 
$\delta t$. However, this introduces a variably-sized time steps, and high-order BDF methods can be unstable with variable
time steps. \todo{Talk about other options?}

The goal here is to design the space-time construction code to be such that the user modifies the right-hand-side appropriately
such that the first step(s) are of an appropriate order of accuracy. This should make the code fairly general / ``non-intrusive.'' 


% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\subsection{Initialization with Runge--Kutta methods}
To start an $s$-step multistep scheme, $s$ starting values, $\mathbf{u}_0, \mathbf{u}_1, \dots, \mathbf{u}_{s-1}$, are required to compute the first unknown $\mathbf{u}_s$. The first of which is provided by the initial condition $\mathbf{u}_0 \equiv \mathbf{u}(0)$, but the remaining $s-1$ of them need to be generated. The typical way to do this is using a one-step, multi-stage RK method (see \S \ref{sec:rk_sequential}) (see L. F. Shampine, Numerical solution of ordinary differential equations, pp. 228--231, 1994). When considering an $\mathcal{O}(k)$-accurate multistep scheme, there are two possibilities for initializing these values (which will retain the overall $\mathcal{O}(k)$ accuracy of the scheme):
\begin{itemize}
\item using an $\mathcal{O}(k-1)$-accurate RK scheme, or
\item using an $\mathcal{O}(k)$-accurate RK scheme
\end{itemize}
The latter option is preferred because it more-or-less results in a global truncation error that would be the same as if the values were initialized with the exact solution.

For explicit multistep, it makes sense to use ERK, and for implicit multistep it makes sense to use IRK. I think that in both of these cases, the multistep stability regions are a subset of those of the RK schemes, so we shouldn't run into stability issues if the RK scheme uses the same time step size as the multistep scheme. 

In terms of implementation: 
\begin{itemize}
\item the first unknown in each space-time linear system will be $\mathbf{u}_s$
\item the first $s$ equations, which are associated with the solution of $\mathbf{u}_s, \mathbf{u}_{s+1}, \dots, \mathbf{u}_{2s-1}$, will need to have their right hand sides modified
\end{itemize}

% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\section{Runge--Kutta methods}\label{sec:rk_sequential}

It's also useful to consider the integration of \eqref{eq:odeM} using Runge--Kutta (RK) methods---especially within the context of initializing multistep methods---, which are one-step, multi-stage methods. These come in both explicit (ERK) and implicit (IRK) varieties. Here, we'll just limit our IRK considerations to so-called diagonal IRK (DIRK) schemes which are defined as IRK schemes having a lower triangular Butcher table. 

In practice, we're only considering second- and third-order integrators which we can obtain with two- and three-stage RK schemes, respectively. If we consider marching from time $t_n$ to $t_{n+1}$, with step length $h = t_{n+1} - t_n$, then the following RK schemes can be used.

\subsection{ERK}
Consider a two- or three-stage ERK scheme having the following Butcher table:
\begin{align}
\begin{array}
{c|ccc}
0 & \\
c_2 & a_{21} & \\
c_3 & a_{31} & a_{32} & \\
\hline
&b_1 & b_2 & b_3
\end{array}
\end{align}
Note that the two-stage method comes from having $c_3 = a_{31} = a_{32} = b_{3} = 0$.
Then we have the scheme:
\begin{align}
\begin{split}
\mathbf{u}_{n+1} &= \mathbf{u}_{n} + h(b_1 \mathbf{k}_1 + b_2 \mathbf{k}_2 + b_3 \mathbf{k}_3), \\
M \mathbf{k}_1 &= -\mathcal{L}(t_n) \mathbf{u}_n + \mathbf{g}(t_n), \\
M \mathbf{k}_2 &= -\mathcal{L}(t_n + c_2 h) [ \mathbf{u}_n + h a_{21} \mathbf{k}_1 ] + \mathbf{g}(t + c_2 h), \\
M \mathbf{k}_3 &= -\mathcal{L}(t_n + c_3 h) [ \mathbf{u}_n + h(a_{31} \mathbf{k}_1 + a_{32} \mathbf{k}_2) ] + \mathbf{g}(t + c_3 h).
\end{split}
\end{align}
The stage vectors $\mathbf{k}_i$ in this scheme must be computed sequentially, and each comes as the result of solving a linear system---except in the event that $M \equiv I$, then they are given explicitly.

\subsection{DIRK}
Consider a two- or three-stage DIRK scheme with the following Butcher table:
\begin{align}
\begin{array}
{c|ccc}
c_1 & a_{11} \\
c_2 & a_{21} & a_{22} \\
c_3 & a_{31} & a_{32} & a_{33} \\
\hline
&b_1 & b_2 & b_3
\end{array}
\end{align}
Note that the two-stage method comes from having $c_3 = a_{31} = a_{32} = a_{33} = b_{3} = 0$.
Then we have the scheme:
\begin{align}
\begin{split}
\mathbf{u}_{n+1} &= \mathbf{u}_{n} + h(b_1 \mathbf{k}_1 + b_2 \mathbf{k}_2 + b_3 \mathbf{k}_3), \\
[M + a_{11} h \mathcal{L}(t_n + c_1 h) ] \mathbf{k}_1 &= -\mathcal{L}(t_n + c_1 h) \mathbf{u}_n + \mathbf{g}(t_n + c_1 h), \\
[M + a_{22} h \mathcal{L}(t_n + c_2 h) ] \mathbf{k}_2 &= -\mathcal{L}(t_n + c_2 h)[ \mathbf{u}_n + h a_{21} \mathbf{k}_1] + \mathbf{g}(t_n + c_2 h), \\
[M + a_{33} h \mathcal{L}(t_n + c_3 h) ] \mathbf{k}_3 &= -\mathcal{L}(t_n + c_3 h)[ \mathbf{u}_n + h(a_{31} \mathbf{k}_1 +  a_{32} \mathbf{k}_2)] + \mathbf{g}(t_n + c_3 h).
\end{split}
\end{align}
The stage vectors $\mathbf{k}_i$ in this scheme must be computed sequentially, and each comes as the result of solving a linear system---even in the event that $M \equiv I$.
 
\tcb{Note that in the case that $\mathcal{L}$ is independent of time (BCs and PDE coefficients are time independent), the matrices multiplying the stage-vectors are time-independent; additionally, if an SDIRK scheme is used then it'll be the same matrix for each stage. This might be more of a practical detail when implementing a sequential time-stepping solver because then the same matrices (or matrix in the case of SDIRK) can be reused at every time level.}


% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\section{Tensor-product structure?}


% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\newpage
\section{Time integration}

Using a method-of-lines approach, we typically formulate an initial value problem (IVP) as an ode in time,
with a general form along the lines of
\begin{align}
\mathbf{u}_t(t) = \mathbf{f}(\mathbf{u}, t) = \mathbf{k}(\mathbf{u},t) + \mathbf{g}(\mathbf{u},t), \label{eq:ode}
\end{align}
where $\mathbf{u}(t)$ is the full spatial solution at time $t$ and we break the right-hand side $\mathbf{f}$ into
temporally stiff terms, $\mathbf{k}$, and non-stiff terms, $\mathbf{g}$. Then, we rewrite \eqref{eq:ode} as
\begin{align}
\mathcal{F}(\mathbf{u},\mathbf{u}_t,t) & = \mathcal{G}(\mathbf{u},t),\label{eq:imp_exp}
\end{align}
where $\mathcal{F}$ is the implicit part of our operator, typically chosen to correspond with the stiff part of \eqref{eq:ode},
which is integrated implicitly, and $\mathcal{G}$ is the explicit part of our operator, which is chosen to correspond with non-stiff
terms in \eqref{eq:ode}. Equation \eqref{eq:imp_exp} is a differential algebraic equation (DAE). The following table, taken from the
PetSc manual, gives a nice overview of how $\mathcal{F}$ and $\mathcal{G}$ are defined for various circumstances: 

%
\begin{table}[h!]
\small
\centering
\begin{tabular}{l l l} \hline
$\mathbf{u}_t = \mathbf{g}(\mathbf{u},t)$  & non-stiff ODE & $\mathcal{F}(\mathbf{u},\mathbf{u}_t,t) = \mathbf{u}_t$, 
	\\&& \hspace{3.5ex}$\mathcal{G}(\mathbf{u},t) = \mathbf{g}(\mathbf{u},t)$ \\
$\mathbf{u}_t = \mathbf{k}(\mathbf{u},t)$  & stiff ODE & $\mathcal{F}(\mathbf{u},\mathbf{u}_t,t) = \mathbf{u}_t - \mathbf{k}(\mathbf{u},t)$, 
	\\&& \hspace{3.5ex}$\mathcal{G}(\mathbf{u},t) = \mathbf{0}$ \\
$M\mathbf{u}_t = \mathbf{g}(\mathbf{u},t)$  & non-stiff ODE with & $\mathcal{F}(\mathbf{u},\mathbf{u}_t,t) = \mathbf{u}_t$, 
	\\& mass matrix& \hspace{3.5ex}$\mathcal{G}(\mathbf{u},t) = M^{-1}\mathbf{g}(\mathbf{u},t)$ \\
$M\mathbf{u}_t = \mathbf{k}(\mathbf{u},t)$  & stiff ODE with mass & $\mathcal{F}(\mathbf{u},\mathbf{u}_t,t) = M\mathbf{u}_t - \mathbf{k}(\mathbf{u},t)$, 
	\\&matrix & \hspace{3.5ex}$\mathcal{G}(\mathbf{u},t) = \mathbf{0}$ \\\hline
$\mathbf{u}_t = \mathbf{k}(\mathbf{u},t) + \mathbf{g}(\mathbf{u},t)$  & stiff/non-stiff ODE &  $\mathcal{F}(\mathbf{u},\mathbf{u}_t,t) =
	\mathbf{u}_t - \mathbf{k}(\mathbf{u},t)$, \\&& \hspace{3.5ex}$\mathcal{G}(\mathbf{u},t) = \mathbf{g}(\mathbf{u},t)$ \\\hline
$M\mathbf{u}_t = \mathbf{k}(\mathbf{u},t) + \mathbf{g}(\mathbf{u},t)$  & stiff/non-stiff ODE &  $\mathcal{F}(\mathbf{u},\mathbf{u}_t,t) =
	M\mathbf{u}_t - \mathbf{k}(\mathbf{u},t)$, \\&with mass matrix& \hspace{3.5ex}$\mathcal{G}(\mathbf{u},t) = M^{-1}\mathbf{g}(\mathbf{u},t)$ \\\hline
\end{tabular}
\end{table}
%

Given the decomposition in \eqref{eq:imp_exp}, many time-stepping schemes can be formulated based on the action of
$\mathcal{G}$ and $\mathcal{F}^{-1}$.

\textbf{Example:} backward Euler\\
\indent Solve for $k = f(\mathbf{x} + \delta t\mathbf{k}, t + \delta t)$ \\
\indent $\mathbf{x} =  \mathbf{x} + \delta t \mathbf{k}$.

%Note, number of iterations with GMRES and no block scaling is, e.g., 225. Block-diagonal scaling reduces
%to 19, AIR reduces to 5. 
%
%
%%%%
%\begin{table}[h!]
%\centering
%{\footnotesize
%\begin{tabular}{| c r r r r r r c c c|} \hline
%order & np & DOFs & loc DOFs & nnz & OC & dt=dx & $\rho$ & it/solve & solve/step \\\hline
%2 & 4 & 110592 & 27648 & 4976640 & 1.41 & 0.0145 & 0.0041 & 4 & 1 \\
%2 & 16 & 442368 & 28224 & 19906560 & 1.39 & 0.0072 & 0.0039 & 4 & 1 \\
%2 & 64 & 1769472 & 27648 & 79626240 & 1.39 & 0.0036 & 0.0042 & 4 & 1 \\
%2 & 256 & 7077888 & 27648 & 318504960 & 1.38 & 0.0018 & 0.0041 & 4 & 1 \\
%3 & 4 & 196608 & 49152 & 15728640 & 1.46 & 0.0145 & 0.0063 & 4 & 2 \\
%3 & 16 & 786432 & 50176 & 62914560 & 1.42 & 0.0072& 0.0069 & 4 & 2 \\
%3 & 64 & 3145728 & 49152 & 251658240 & 1.41 & 0.0036 & 0.0068 & 4 & 2 \\
%3 & 256 & 12582912 & 49152 & 1006632960 & 1.39 & 0.0018 & 0.0067 & 4 & 2 \\
%4 & 4 & 307200 & 76800 & 38400000 & 1.61 & 0.0145 & 0.0111 & 5 & 3 \\
%4 & 16 & 1228800 & 78400 & 153600000 & 1.53 & 0.0072 & 0.0097 & 4 & 3 \\
%4 & 64 & 4915200 & 76800 & 614400000 & 1.48 & 0.0036 & 0.0121 & 5 & 3 \\
%4 & 256 & 19660800 & 76800 & -1837367296 & 1.46 & 0.0018 & 0.0098 & 4 & 3 \\\hline
%\end{tabular}
%}
%\caption{Results applying AIR to implicit time-stepping schemes with DG advection (MFEM ex. 9). 
%Here, we use A-stable SDIRK time integration schemes, and pick the order of time integration equal to
%the order of spatial discretization, and set $dt = dx$, to get the same accuracy in space and time. Solves
%are done without Krylov acceleration, with a relative residual tolerance of $10^{-8}$. Higher-order SDIRK
%schemes require solving multiple linear systems, as shown in the solves/step column. Convergence factors
%are $< 0.012$ in all iterations here. Convergence can degrade if we increase, for example, $dt \gg \sqrt{dx}$.
%In general such a scenario is not likely to come up; however, for parallel-in-time with MGRiT, this may come up
%if we were to coarsen in time and not in space, and we will have to test if spatial coarsening is necessary.}
%\end{table}



% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\newpage
\section{Block AIR in time domain}

Consider some linear multistep method as discussed in Section \ref{sec:disc}. Let us consider schemes
with at most four stages (e.g., BDF3 or AM4). The space-time matrix will take the form
%
\begin{align*}
A = 
\begin{pmatrix}
	A_{1} \\ 
	B_{1} & A_2 \\ 
	C_1 & B_2 & A_3 \\
	D_1 & C_2 & B_3 & A_4 \\
	& \ddots & \ddots & \ddots & \ddots \\
	& & D_{n-3} & C_{n-2} & B_{n-1} & A_n
\end{pmatrix},
\end{align*}
%
for some sparse matrices $A_i,B_i,C_d,D_i$. In such temporal discretizations, the sub-diagonal is
almost always the largest ``weight'' in a given row. Because of this, semi-coarsening in the time
dimension is a natural choice. Without loss of generality, suppose $n$ is even, and we choose the
first row as an F-point. Then, sub-matrices take the following form:
%
\begin{align*}
A_{ff} & = 
\begin{pmatrix}
	A_{1} \\ 
	C_1 & A_3 \\
	& C_3 & A_5 \\
	& & \ddots & \ddots  \\
	& & & C_{n-3} & A_{n-1}
\end{pmatrix},
\hspace{6ex}
A_{fc} = 
\begin{pmatrix}
	 \mathbf{0} & \\
	B_2 & \ddots\\
	D_2 & B_4  \\
	& D_4 & B_6  \\
	& & \ddots & \ddots & \ddots\\
	& & & D_{n-4} & B_{n-2} &  \mathbf{0}
\end{pmatrix},
\\
A_{cf} & = 
\begin{pmatrix}
	B_{1} \\ 
	D_1 & B_3 \\
	& D_3 & B_5 \\
	&& \ddots & \ddots \\
	&& & D_{n-3} & B_{n-1}
\end{pmatrix},
\hspace{4ex}
A_{cc} = 
\begin{pmatrix}
	A_{2} \\ 
	C_2 & A_4 \\
	& C_4 & A_6 \\
	& & \ddots & \ddots  \\
	& & & C_{n-2} & A_{n}
\end{pmatrix}.
\end{align*}
%
Note that we could also swap C-points and F-points. This would lead to a different framework, which will be
explored later. Let $A_{ff} = D_{ff} + L_{ff} = (I - (-L_{ff})D_{ff}^{-1})D_{ff}$. Then, a Neumann approximation 
can be used to approximate $A_{ff}^{-1}$ and an approximation to $R_{\textnormal{ideal}}$ given by
%
\begin{align*}
R = \begin{pmatrix} -A_{cf}\mathcal{D}_{ff}^{-1}(I - L_{ff}D_{ff}^{-1} + (L_{ff}D_{ff}^{-1})^2 + ...) & I\end{pmatrix},
\end{align*}
%
where $\mathcal{D}_{ff}$ is the diagonal block of $A_{ff}$. 
Let $Z_i$ denote the F-point block of $R$ for a degree-$i$ truncated Neumann approximation. Then
%
{\footnotesize
\begin{align*}
Z_0 & = -
\begin{pmatrix}
	B_{1}A_1^{-1} \\ 
	D_1A_1^{-1} & B_3A_3^{-1} \\
	& D_3A_3^{-1} & B_5A_5^{-1} \\
	&& \ddots & \ddots \\
	&& & D_{n-3}A_{n-3}^{-1} & B_{n-1}A_{n-1}^{-1}
\end{pmatrix}, \\
Z_1& = -
\begin{pmatrix}
	B_{1}A_1^{-1} \\ 
	D_1A_1^{-1} & B_3A_3^{-1} \\
	& D_3A_3^{-1} & B_5A_5^{-1} \\
	&& \ddots & \ddots \\
	&& & D_{n-3}A_{n-3}^{-1} & B_{n-1}A_{n-1}^{-1}
\end{pmatrix}
\begin{pmatrix}
	I \\ 
	-C_1A_1^{-1} & I \\
	& -C_3A_3^{-1} & I \\
	& & \ddots & \ddots  \\
	& & & -C_{n-3}A_{n-3}^{-1} & I
\end{pmatrix} \\
& = -
\begin{pmatrix}
	B_{1}A_1^{-1} \\ 
	D_1A_1^{-1} - B_3A_3^{-1}C_1A_1^{-1} & B_3A_3^{-1} \\
	-D_3A_3^{-1}C_1A_1^{-1} & D_3A_3^{-1} - B_5A_5^{-1}C_3A_3^{-1} & B_5A_5^{-1} \\
	& \ddots & \ddots & \ddots \\
%	& & D_{n-3}A_{n-3}^{-1}C_{n-5} & D_{n-3}A_{n-3}^{-1} + B_{n-1}A_{n-1}^{-1}C_{n-3} & B_{n-1}A_{n-1}^{-1}
\end{pmatrix}
\end{align*}
}%
For ease of notation and also likely important for a clean derivation, suppose we have a PDE with no
time-dependent coefficients. This means $A_1 = A_2 = ...$, $B_1 = B_2 = ...$, etc. \todo{What about
boundary conditions? Assuming we can move to rhs; need to verify.} Then
%
\begin{align*}
Z_0 & = -
\begin{pmatrix}
	BA^{-1} \\ 
	DA^{-1} & BA^{-1} \\
	& \ddots & \ddots \\
	& & DA^{-1} & BA^{-1}
\end{pmatrix}, \\
Z_1 & = -
\begin{pmatrix}
	BA^{-1} \\ 
	DA^{-1} - BA^{-1}CA^{-1} & BA^{-1} \\
	-DA^{-1}CA^{-1} & DA^{-1} - BA^{-1}CA^{-1} & BA^{-1} \\
	& \ddots & \ddots & \ddots \\
	& & -DA^{-1}CA^{-1} & DA^{-1} - BA^{-1}CA^{-1} & BA^{-1}
\end{pmatrix}
\end{align*}
%
Now, let us define $P$ as the famous one-point interpolation that has worked well for AIR, where we
interpolate each F-point from its strongest C-neighbor:
%
\begin{align*}
P = \begin{pmatrix} W \\ I \end{pmatrix} = 
\begin{pmatrix}
\mathbf{0} \\ 
I & \mathbf{0} \\
& \ddots & \ddots\\
& & I & \mathbf{0} \\ \hline
I \\
& \ddots \\
& & \ddots \\
& & & I
\end{pmatrix}
\end{align*}
%
Computing $\mathcal{K} := RAP$ in block form, we have $\mathcal{K} = RAP = ZA_{ff}W + A_{cf}W 
+ ZA_{fc}+A_{cc}$. Looking at each of these terms, we have
%
\begin{align*}
A_{cc} &= 
\begin{pmatrix}
	A \\ 
	C & A \\
	& \ddots & \ddots  \\
	& & C & A
\end{pmatrix},
\\
Z_0A_{fc} &=-
\begin{pmatrix}
	\mathbf{0} \\
	BA^{-1}B  & \mathbf{0} \\ 
	DA^{-1}B + BA^{-1}D & BA^{-1}B & \mathbf{0} \\
	& \ddots & \ddots & \ddots \\
	& & DA^{-1}B + BA^{-1}D & BA^{-1}B & \mathbf{0} 
\end{pmatrix},
\\
Z_1A_{fc} & =
\begin{pmatrix}
0
\end{pmatrix}, 
\\
A_{cf}W & = 
\begin{pmatrix}
	\mathbf{0} \\
	B  & \mathbf{0} \\ 
	D & B & \mathbf{0} \\
	& \ddots & \ddots & \ddots \\
	& & D & B & \mathbf{0} 
\end{pmatrix},
\hspace{4ex}
A_{ff}W = 
\begin{pmatrix}
	\mathbf{0} \\
	A & \mathbf{0} \\ 
	C & A & \mathbf{0}\\
	& \ddots & \ddots & \ddots  \\
	& & C & A & \mathbf{0}
\end{pmatrix},
\\
Z_0A_{ff}W & = -
\begin{pmatrix}
	\mathbf{0} \\
	B & \mathbf{0} \\ 
	D + BA^{-1}C & B & \mathbf{0} \\
	DA^{-1}C & D + BA^{-1}C & B & \mathbf{0} \\
	& \ddots & \ddots & \ddots & \ddots \\
	& & DA^{-1}C & D + BA^{-1}C & B & \mathbf{0}
\end{pmatrix},
\\
Z_1A_{ff}W & = -
\begin{pmatrix}
	\mathbf{0} \\
	B & \mathbf{0} \\ 
	D & B & \mathbf{0} \\
	-BA^{-1}CA^{-1}C & D & B & \mathbf{0} \\
	-DA^{-1}CA^{-1}C & -BA^{-1}CA^{-1}C & D & B & \mathbf{0} \\
	& \ddots & \ddots & \ddots & \ddots & \ddots \\
\end{pmatrix},
\end{align*}
%


Coarse-grid operators then take the form
%
\begin{align*}
\mathcal{K}_0 & =
\begin{pmatrix}
	A \\
	C - BA^{-1}B& A \\ 
	-BA^{-1}C & C- BA^{-1}B & A \\
	- DA^{-1}C & - BA^{-1}C & C - BA^{-1}B & A \\
	& \ddots & \ddots & \ddots & \ddots \\
	& & - DA^{-1}C & -BA^{-1}C & C - BA^{-1}B & A
\end{pmatrix},
\end{align*}
%
\tcb{$\mathcal{K}_0$ isn't quite finished.}





% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\newpage
\section{Runge-Kutta}\label{sec:RK}

% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\subsection{RK4:} 

Consider the classic RK4 time integration scheme as an example to solve $\mathbf{u}_t = \mathcal{L}\mathbf{u} + \mathbf{g}(t)$,
where $\mathcal{L}$ is a spatial discretization matrix and $\mathbf{u}_n$ the spatial solution in vector form at time $t_n$. 
Then, the four stages can be written out as:
%
\begin{align*}
k_1 & = \delta t\mathcal{L}\mathbf{u}_n + \delta t\mathbf{g}(t_n), \\
k_2 & = \delta t\mathcal{L}(\mathbf{u}_n + \tfrac{k_1}{2}) + \delta t\mathbf{g}(t_{n+\frac{1}{2}}) \\
	& = \delta t\mathcal{L}\mathbf{u}_n + \tfrac{\delta t^2}{2}\mathcal{L}^2\mathbf{u}_n + \delta t\mathbf{g}(t_{n+\frac{1}{2}}) +
	\tfrac{\delta t^2}{2}\mathcal{L}\mathbf{g}(t_n) ,\\
k_3 & = \delta t\mathcal{L}(\mathbf{u}_n + \tfrac{k_2}{2}) + \delta t\mathbf{g}(t_{n+\frac{1}{2}}) \\
	& = \delta t\mathcal{L}\mathbf{u}_n + \tfrac{\delta t^2}{2}\mathcal{L}^2\mathbf{u}_n + \tfrac{\delta t^3}{4}\mathcal{L}^3\mathbf{u}_n +
	\delta t(I+\tfrac{\delta t}{2}\mathcal{L})\mathbf{g}(t_{n+\frac{1}{2}}) + \tfrac{\delta t^3}{4}\mathcal{L}^2\mathbf{g}(t_n), \\
k_4 & = \delta t\mathcal{L}(\mathbf{u}_n + k_3) + \delta t \mathbf{g}(t_{n+1}), \\
	& = \delta t\mathcal{L}\mathbf{u}_n + \delta t^2\mathcal{L}^2\mathbf{u}_n + \tfrac{\delta t^3}{2}\mathcal{L}^3\mathbf{u}_n +
	\tfrac{\delta t^4}{4}\mathcal{L}^4\mathbf{u}_n + \delta t^2\mathcal{L}(I+\tfrac{\delta t}{2}\mathcal{L})\mathbf{g}(t_{n+\frac{1}{2}}) +
	 ...\\&\hspace{4ex} \tfrac{\delta t^4}{4}\mathcal{L}^3\mathbf{g}(t_n) + \delta t \mathbf{g}(t_{n+1}),
\end{align*}
%
where 
%
\begin{align*}
\mathbf{u}_{n+1} & = \mathbf{u}_n + \tfrac{1}{6}(k_1+2k_2+2k_3+k_4).
\end{align*}
%
These stages can expressed algebraically as a space-time matrix. For RK4, such a matrix takes the block form:
%
{\footnotesize
\begin{align}
\begin{split}
%\begin{pmatrix} I &  \\ -\mathcal{S} & I \end{pmatrix}\hspace{3ex}\sim\hspace{3ex}
\begin{pmatrix}
\ddots \\ 
-\tfrac{1}{6}I &I &  &  &  &   & \\
&-\delta t \mathcal{L} & I &  &  &  &  \\
&-\delta t \mathcal{L} & -\tfrac{\delta t}{2} \mathcal{L} & I &   &  & \\
&-\delta t \mathcal{L} &  & -\tfrac{\delta t}{2} \mathcal{L} & I  &  & \\
&-\delta t \mathcal{L} &  &  & -\delta t \mathcal{L} & I &  \\
&-I & -\frac{1}{6}I & -\frac{1}{3}I & -\frac{1}{3}I & -\frac{1}{6}I & I \\
&&&&&& -\delta t \mathcal{L} & \ddots
\end{pmatrix}
\begin{pmatrix} \vdots \\ \mathbf{u}_n \\ k_1^{(n)} \\ k_2^{(n)} \\ k_3^{(n)} \\ k_4^{(n)} \\ \mathbf{u}_{n+1}  \\ \vdots \end{pmatrix}
= \\
\begin{pmatrix} \vdots \\ \mathbf{0} \\ \delta t\mathbf{g}(t_n) \\  \delta t\mathbf{g}(t_{n+\frac{1}{2}}) +
	\tfrac{\delta t^2}{2}\mathcal{L}\mathbf{g}(t_n) \\ \delta t(I+\tfrac{\delta t}{2}\mathcal{L})\mathbf{g}(t_{n+\frac{1}{2}}) +
	\tfrac{\delta t^3}{4}\mathcal{L}^2\mathbf{g}(t_n) \\ \delta t^2\mathcal{L}(I+\tfrac{\delta t}{2}\mathcal{L})\mathbf{g}(t_{n+\frac{1}{2}}) +
	\tfrac{\delta t^4}{4}\mathcal{L}^3\mathbf{g}(t_n) + \delta t \mathbf{g}(t_{n+1}) \\ \mathbf{0}  \\ \vdots
\end{pmatrix}
\end{split}\label{eq:s_stage}
\end{align}
%
}Going from the expanded $s$-stage form to a condensed $2\times 2$ block matrix can be achieved by straightforward
elimination of variables $k_1,k_2,...$. Combining, we have 
%
\begin{align*}
\mathbf{u}_{n+1} & = \mathbf{u}_n + \tfrac{1}{6}(k_1+2k_2+2k_3+k_4) \\
	& = \mathcal{S}\mathbf{u}_n + \mathcal{G}_n, \\
\mathcal{S} & = I + \delta t\mathcal{L} + \tfrac{\delta t^2}{2}\mathcal{L}^2 + \tfrac{\delta t^3}{6}\mathcal{L}^3 +
	\tfrac{\delta t^4}{24}\mathcal{L}^4, \\
\mathcal{G}_n & = \tfrac{\delta t}{6}\Big[ I  +\delta t \mathcal{L} + \tfrac{\delta t^2}{2}\mathcal{L}^2 + \tfrac{\delta t^3}{4}\mathcal{L}^3\Big] \mathbf{g}(t_{n}) +
	\tfrac{\delta t}{3}\Big[2I + \delta t \mathcal{L} + \tfrac{\delta t^2}{4}\mathcal{L}^2\Big]\mathbf{g}(t_{n+\frac{1}{2}}) + \tfrac{\delta t}{6} \mathbf{g}(t_{n+1}).
\end{align*}
%
In matrix form, this looks like
%
\begin{align} \label{eq:condensed}
\begin{pmatrix} 
I \\ -\mathcal{S} & I \\ & -\mathcal{S} & I \\ && \ddots & \ddots \end{pmatrix} \begin{pmatrix} \mathbf{u}_0 \\ \mathbf{u}_1 \\\mathbf{u}_2 \\ \vdots \end{pmatrix}
	= \begin{pmatrix} \mathcal{G}_0 \\ \mathcal{G}_1 \\ \mathcal{G}_2 \\ \vdots \end{pmatrix}.
\end{align}
%
Note that all explicit RK methods will take such a form, where $\mathcal{S}$ consists of some polynomial in $\mathcal{L}$.
\tcb{This right-hand side seems quite complicated to this. Does MGRiT arrive at same structure? I suppose this is in the user's hands...}

By the single sub-diagonal structure here, we can actually form $R_{ideal}$. In particular, operators take the form
%
\begin{align}
R_{ideal} & = \begin{pmatrix}-\mathcal{S}^2 & & & I & \\ & -\mathcal{S}^2 & & & I \\ & & \ddots & & & \ddots\end{pmatrix} \label{eq:ridealS}\\
R_{ideal}AP & = \begin{pmatrix} I \\ -\mathcal{S}^2 & I \\ & -\mathcal{S}^2 & I \\ & & \ddots & \ddots \end{pmatrix}.\label{eq:rapS}
\end{align}
%
Note that this holds regardless of interpolation to F-points. In fact, the restriction only to C-points as used in MGRiT actually makes
sense if you form $R_{ideal}$ because that is the $\ell^2$-orthogonal projection. \tcb{Has MGRiT been tried with $R_{ideal}$ instead
of $P_{ideal}$? In all AIR tests $R_{ideal}$ has performed better; it is also setting the error equal to zero at C-points, which is typically preferable
to the residual, which is achieved by $P_{ideal}$..} In any case, notice this is exactly as arrived at in
MGRiT, albeit in explicit algebraic form: the coarse-grid operator consists of taking two steps, $\mathcal{S}^2$, using the fine-grid
time stepper. Denote $\mathcal{S}_{\delta t}$ as $\mathcal{S}$ with time step $\delta t$. Expanding, 
%
\begin{align*}
\mathcal{S}_{\delta t}^2 & = \Big( I + \delta t\mathcal{L} + \tfrac{\delta t^2}{2}\mathcal{L}^2 + \tfrac{\delta t^3}{6}\mathcal{L}^3 +
	\tfrac{\delta t^4}{24}\mathcal{L}^4\Big)^2 \\
& = I + 2 \delta t \mathcal{L} + 2 \delta t^2\mathcal{L}^2 +  \tfrac{4\delta t^3}{3}\mathcal{L}^3 + \tfrac{2\delta t^4}{3}\mathcal{L}^4 + \tfrac{\delta t^5}{4}\mathcal{L}^5 + \tfrac{5\delta t^6}{72}\mathcal{L}^6 + \tfrac{\delta t^7}{72}\mathcal{L}^7 + \tfrac{\delta t^8}{576} \mathcal{L}^8 \\
& = \mathcal{S}_{2\delta t} + O((\delta t\mathcal{L})^5).
\end{align*}
%
That is, taking a time step of size $2\delta t$, as done in MGRiT, is, in the Taylor-sense, the best approximation to two steps on 
the fine grid. However, this assumes that $\|\delta t\mathcal{L}\| < 1$. \tcb{Is this more or less enforced by the stability criterion
of explicit time integrators? Also, is this unique to RK4, or for explicit RK is it generally the case that taking a time step of $2\delta t$
is, in a Taylor-sense, the best approximation to two time steps?}

% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\subsection{General RK schemes}

Returning to \eqref{eq:s_stage}, one can notice that the block matrix resembles that of the Butcher Tableaux for RK4. 
Implicit schemes generally cannot be reduced to a condensed block form, as in \eqref{eq:condensed}. However, given the Butcher
Tablueax for an $s$-stage Runge-Kutta scheme, implicit or explicit, the matrix will take on a block structure as in \eqref{eq:s_stage}
with the following blocks (overlapping other blocks in the first and last row/column):
%
\begin{align} \label{eq:gen_block}
\begin{pmatrix} 
I  & \mathbf{0} & \mathbf{0} & ... & \mathbf{0} & \mathbf{0}\\
-\delta t\mathcal{L} & I - a_{11}\delta t\mathcal{L} & -a_{12}\delta t\mathcal{L} & ... &  -a_{1s}\delta t\mathcal{L} & \mathbf{0} \\
-\delta t\mathcal{L} & -a_{21}\delta t\mathcal{L} & I -a_{22}\delta t\mathcal{L} & ... & -a_{2s}\delta t\mathcal{L} & \mathbf{0} \\ 
\vdots  & \vdots & \vdots & \ddots & \vdots  & \vdots \\
-\delta t\mathcal{L} & -a_{s1}\delta t\mathcal{L} & -a_{s2}\delta t\mathcal{L} & ...  & I - a_{ss}\delta t\mathcal{L} & \mathbf{0} \\ 
-I & -b_1I & -b_2I & ... & -b_s I & I
\end{pmatrix}
\begin{pmatrix} \mathbf{u}_n \\ \mathbf{k}_1^{(n)} \\ \mathbf{k}_2^{(n)} \\ \vdots \\ \mathbf{k}_s^{(n)} \\ \mathbf{u}_{n+1} \end{pmatrix} =
\end{align}
%
For diagonally implicit Runge-Kutta (DIRK) methods, the blocks in \eqref{eq:gen_block} are lower triangular. For explicit
methods, they are lower triangular with unit diagonal. This allows for a natural elimination of middle stages for the condensed
form in \eqref{eq:condensed}. \tcb{Would such a matrix be algebraically amenable to AIR-type algorithms? Unfortunately, the
number of rows in the matrix scales with $s$...} We can also condense \eqref{eq:gen_block} using Kronecker products for a
block matrix structure:
%
\begin{align*}
\begin{pmatrix}
I_{n\times n} &  \\
-\delta t(\mathbf{1}_n \otimes \mathcal{L}) & I_{ns\times ns} - \delta t A\otimes \mathcal{L} &\\
-I_{n\times n} & -\mathbf{b}\otimes I_{n\times n} & I_{n\times n} \\ 
& & -\delta t(\mathbf{1}_n \otimes \mathcal{L}) & I_{ns\times ns} - \delta t A\otimes \mathcal{L} &  \\
& & -I_{n\times n} & -\mathbf{b}\otimes I_{n\times n} & I_{n\times n} \\
&&& \ddots\text{\hspace{4ex}}  & \ddots\text{\hspace{8ex}}  & \ddots
\end{pmatrix}.
\end{align*}
%
In the spirit of algebraic reduction, suppose we designate solution blocks (with identity on diagonal) as C-points and
the $s$-stage blocks as F-points. Then,
%
{\footnotesize
\begin{align}
-A_{cf}A_{ff}^{-1} & = \begin{pmatrix} \mathbf{0} \\ -(\mathbf{b}\otimes I )(I - \delta tA\otimes \mathcal{L})^{-1} \\
&  -(\mathbf{b}\otimes I) (I - \delta tA\otimes \mathcal{L})^{-1} \\ & & \ddots \end{pmatrix} , \label{eq:acfaff}\\
R_{ideal}AP & =  \begin{pmatrix} I \\ -I -\delta t(\mathbf{b}\otimes I )(I - \delta tA\otimes \mathcal{L})^{-1}(\mathbf{1}_n \otimes \mathcal{L})  & I \\
& -I - \delta t(\mathbf{b}\otimes I )(I - \delta tA\otimes \mathcal{L})^{-1}(\mathbf{1}_n \otimes \mathcal{L})  & I \\ & & \ddots & \ddots \end{pmatrix}.\label{eq:rap}
\end{align}
%
}Notice that this now takes on a form that we can apply a more traditional reduction to with an identity on the diagonal. $A_{ff}^{-1}$ is
block diagonal and invertible. In the case of DIRK methods, $ (I - \delta tA\otimes \mathcal{L})^{-1} $ can be inverted through a block
forward solve, almost like F-relaxation through Runge-Kutta stages. For non-DIRK methods, solving should still be feasible, but is more
complicated. \footnote{
We could also swap the choice of C- and F-points. Then, $A_{ff}^{-1}$ can be directly computed, but is a global, dense lower triangular
matrix coupling all DOFs.} In any case, we are only interested in the solution at specific points in time, corresponding to C-points, and
do not need the intermediate RK stages. Because we can exactly compute $R_{ideal}$, which gives an exact solution at C-points, all we
need now is to solve the coarse-grid problem. Thus, we have projected the problem to dimension independent of $s$ (kind of, although
the diagonal of $A_{ff}$ interpolates to a large problem, inverts there, and brings it back; this particularly motivates DIRK methods). 
One idea is to apply a reduction-based multigrid, exactly as we can do with explicit RK methods.

Coarse-grid correction for AMG takes the form
%
\begin{align*}
\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + P(RAP)^{-1}R(b - A\mathbf{x}^{(k)}).
\end{align*}
%
Recall we are only interested in the solution at C-points, which is obtained in one iteration if $R = R_{ideal}$ and we
solve the coarse-grid exactly. Thus, suppose we take a zero initial guess on C- and F-points. Then the exact solution
at C-points is obtained by solving
%
\begin{align*}
(RAP)\overline{\mathbf{x}}_c & = \mathbf{b}_c - A_{cf}A_{ff}^{-1}\mathbf{b}_f,
\end{align*}
%
where $RAP$ is as in \eqref{eq:rap}, $A_{cf}A_{ff}^{-1}$ in \eqref{eq:acfaff}, and $\mathbf{b}$ is the right-hand side
as determined by the RK discretization. Here, we can apply an operator-based reduction method exactly as in \eqref{eq:ridealS}
and \eqref{eq:rapS}, except now 
%
\begin{align*}
\mathcal{S} & = I + \delta t(\mathbf{b}\otimes I )(I - \delta tA\otimes \mathcal{L})^{-1}(\mathbf{1}_n \otimes \mathcal{L}), \\
\mathcal{S}^2 & = \Big[ I + \delta t(\mathbf{b}\otimes I )(I - \delta tA\otimes \mathcal{L})^{-1}(\mathbf{1}_n \otimes \mathcal{L}) \Big]^2 \\
	 & = I + 2\delta t (\mathbf{b}\otimes I )(I - \delta tA\otimes \mathcal{L})^{-1}(\mathbf{1}_n \otimes \mathcal{L}) + \delta t^2\Big[(\mathbf{b}\otimes I )(I - \delta tA\otimes \mathcal{L})^{-1}(\mathbf{1}_n \otimes \mathcal{L}) \Big]^2.
\end{align*}
%



\tcb{What's interesting is the $\delta t$ inside of $(I - \delta tA\otimes\mathcal{L})^{-1}$ (as opposed to $2\delta t$).
Is this a flaw in the derivation approach? Is it better to do a coarse-grid time step with $2\delta t$ everywhere, or to actually use
$\delta t$ to advance the $s$ stages and $2\delta t$ for the time step??
\begin{itemize}
\item It could actually be of great benefit if we could use the same $\delta t$ for every time internal time step. In particular, if we
use an SDIRK scheme, this means we would only need one spatial solver for all time steps on all time levels. The memory
requirements to have a solver for every time step size are definitely a limiting factor of the MGRiT framework for implicit, and this
would help that situation significantly. 
\item In the case of an explicit scheme, what does this mean? The actual propagation will be computed with $\delta t$, which,
suppose satisfies the stability constraint. If we just extrapolate this to step size $2\delta t$, can we get around the stability
constraint of hyperbolic problems? I feel like this could be promising, but hard to say at this point...
\end{itemize}
}

% ------------------------------------------------------------------------------------------------------- %
\subsection{Aggressive coarsening}

In MGRiT, the speedup is typically better for more aggressive coarsening routines, raising the natural question if we can
coarsen aggressively here and maintain matrix structure. For ease of notation, denote the sub-diagonal in \eqref{eq:rap}
as $L = -I - \delta t(\mathbf{b}\otimes I )(I - \delta tA\otimes \mathcal{L})^{-1}(\mathbf{1}_n \otimes \mathcal{L})$. We will
see that we can coarsen aggressively in a structured manner and retain the matrix structure in \eqref{eq:rap}.


% ------------------------------------------------------------------------------------------------------- %
\subsubsection{$3\times$ coarsening}

%
\begin{align*}
A & = \begin{blockarray}{cccccccc}
& C & F & F & C & F & F & ... \\
\begin{block}{c(ccccccc)}
C &  I & & & & & \\
F &  L & I & & & & \\
F &  & L & I & & & \\
C &  & & L & I & & \\
F &  & & & L & I &  \\
F &  &  & & & L & I  \\ 
\vdots  & & &  & & & \ddots & \ddots \\
\end{block}
\end{blockarray}, 
\hspace{4ex}
A_{fc} = \begin{pmatrix} L \\ \mathbf{0} & \mathbf{0}  \\ & L \\ & \mathbf{0} & \mathbf{0} \\ & & L & \mathbf{0} \\ &&&\ddots&\ddots \end{pmatrix}, 
\\
A_{cf} & = \begin{pmatrix} \mathbf{0} \\ \mathbf{0} & L \\ & \mathbf{0} & \mathbf{0} & L \\ &&& \mathbf{0} & \mathbf{0} & L \\ &&&&&\ddots&\ddots \end{pmatrix}, 
\hspace{10ex}
A_{ff}^{-1} = \begin{pmatrix} I \\ -L & I \\ & & I \\ & & -L & I \\ & & & \ddots & \ddots \end{pmatrix}.
\end{align*}
%
Then, in F-C block ordering, 
%
\begin{align*}
R_{ideal} & = \begin{pmatrix} \mathbf{0} & & & & & \Big| & I \\ -L^2 & L & & & & \Big| & & I \\ & & -L^2 & L & & \Big| &&  & I \\ & & & \ddots & \ddots & \Big| && & & \ddots\end{pmatrix}, \\
R_{ideal}AP & = A_{cc} - A_{cf}A_{ff}^{-1}A_{fc} \\
& = \begin{pmatrix} I \\ & I \\ & & I \\ & & & \ddots\end{pmatrix} - \begin{pmatrix} \mathbf{0} \\ -L^2 & L \\ & & -L^2 & L \\ & & & \ddots & \ddots\end{pmatrix}
	 \begin{pmatrix} L \\ \mathbf{0} & \mathbf{0}  \\ & L \\ & \mathbf{0} & \mathbf{0} \\ & & L & \mathbf{0} \\ &&&\ddots&\ddots \end{pmatrix} \\
& = \begin{pmatrix} I \\ L^3 & I \\ & L^3 & I \\ && \ddots &\ddots\end{pmatrix}.
\end{align*}
%
Notice that we obtain the exact structure as in \eqref{eq:rapS} and \eqref{eq:rap}, now taking three time steps via $L^3$.

% ------------------------------------------------------------------------------------------------------- %
\subsubsection{$4\times$ coarsening}

%
{\footnotesize
\begin{align*}
A & = \begin{blockarray}{ccccccccccc}
& C & F & F & F & C & F & F & F & ... \\
\begin{block}{c(cccccccccc)}
C &  I & & & & & \\
F &  L & I & & & & \\
F &  & L & I & & & \\
F &  & & L & I & & \\
C &  & & & L & I &  \\
F &  &  & & & L & I  \\ 
F & & &  & & & L & I  \\ 
F & & & & & & & L & I  \\ 
\vdots  & & &&& & & & \ddots & \ddots \\
\end{block}
\end{blockarray}, 
\hspace{4ex}
A_{fc} =  \begin{pmatrix} L \\ \mathbf{0} & \mathbf{0}  \\ & \mathbf{0}  \\ & L \\ & \mathbf{0} & \mathbf{0} \\ & & \mathbf{0} \\
	& & L & \mathbf{0} \\ &&&\ddots&\ddots \end{pmatrix}, 
\\
A_{cf} & = \begin{pmatrix} \mathbf{0} \\ \mathbf{0} & \mathbf{0} & L \\ && \mathbf{0} & \mathbf{0} & \mathbf{0} & L \\
	&& &&& \mathbf{0} & \mathbf{0} & \mathbf{0} & L \\ &&&&&&&&\ddots&\ddots \end{pmatrix}, 
\hspace{10ex}
A_{ff}^{-1} = \begin{pmatrix} I \\ -L & I \\ L^2 & -L & I \\ & & & I \\ & & & -L & I \\ & & & L^2 & -L & I\\ & & & & \ddots & \ddots & \ddots \end{pmatrix}.
\end{align*}
}
%
Then, in F-C block ordering, 
%
\begin{align*}
R_{ideal} & = \begin{pmatrix} \mathbf{0} & & & & & & & \Big| & I \\ L^3 & -L^2 & L & & & & & \Big| & & I \\ & & & L^3 & -L^2 & L & & \Big| & & & I
	\\ & & & & & \ddots & \ddots & \Big| && && & \ddots\end{pmatrix}, \\
R_{ideal}AP & = A_{cc} - A_{cf}A_{ff}^{-1}A_{fc} \\
& = \begin{pmatrix} I \\ & I \\ & & I \\ & & & \ddots\end{pmatrix} - \begin{pmatrix} \mathbf{0} \\ L^3 & -L^2 & L \\ & & & L^3 & -L^2 & L \\ & & & &  & \ddots & \ddots\end{pmatrix}
	 \begin{pmatrix} L \\ \mathbf{0} & \mathbf{0}  \\ & \mathbf{0}  \\ & L \\ & \mathbf{0} & \mathbf{0} \\ & & \mathbf{0} \\
	& & L & \mathbf{0} \\ &&&\ddots&\ddots \end{pmatrix}\\
& = \begin{pmatrix} I \\ L^4 & I \\ & L^4 & I \\ && \ddots &\ddots\end{pmatrix}.
\end{align*}
%

In general, the same result will hold for further aggressive coarsening schemes. Note that we \textit{do not} need to compute $R_{ideal}$.
Because we know that $RAP = S_A$, we can simply compute the action of $R_{ideal}$ through F-relaxation (equivalent to $A_{ff}^{-1}$)
and applying $A_{cf}$ to the F-points. \tcb{Without considering the choice of coarse grid, is this equivalent to MGRiT? Need to work out if
the $\Phi$ operator in MGRiT actually takes the form of the subdiagonal $L$ in \eqref{eq:rap}. If not, we should work out algebraically what
MGRiT does algebraically for a RK scheme to see how it compares.}


\end{document}







